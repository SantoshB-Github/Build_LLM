{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUXGkmF/EVkOANtjr5R10V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantoshB-Github/Build_LLM/blob/main/LLM_Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCN9T7W2iLLW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel (nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"ctx_len\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout (cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Seqential (*[DummyTransformerBlock(cfg) for _ in range (cfg[\"n_layers\"]) ]) #Use a placeholder for TransformerBlock\n",
        "\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])      #Use a placeholder for LayerNorm\n",
        "    self.out_head = nn.Linear (cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
        "\n",
        "  def forward (self, in_idx):\n",
        "    batch_size, seq_len=in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
        "\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):   #A simple placeholder class that will be replaced by a real TransformerBlock later\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):                   # This block does nothing and just returns its input.\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):          #  A simple placeholder class that will be replaced by a real TransformerBlock later\n",
        "  def __init(self, normalized_shape, eps = 1e-5):    #The parameters here are just to mimic the LayerNorm interface.\n",
        "    super().__init__()\n",
        "\n",
        "  def forward (self, x):\n",
        "    return x"
      ]
    }
  ]
}